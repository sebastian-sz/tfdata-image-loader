{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tfdata_loader_colab_benchmark_loaders.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrooAAAqBswK",
        "colab_type": "text"
      },
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i52jJdVRAO7p",
        "colab_type": "text"
      },
      "source": [
        "##### Experiment setup:\n",
        "In this notebook I'm presenting a time benchmark which compares the three options to load images for Image Classification task:\n",
        "1. `tfdata-image-loader` (my repo) \n",
        "2. The default Keras `flow_from_directory` option,\n",
        "3. The new keras.preprocessing `image_dataset_from_directory`\n",
        "\n",
        "In order to perform the measurements I'm fine tuning a MobilenetV2 head on a new set of classes.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoYSMbdhua4F",
        "colab_type": "text"
      },
      "source": [
        "##### Experiment assumptions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wi4DUa2ufIM",
        "colab_type": "text"
      },
      "source": [
        "In order to train MobilenetV2 we want to to the following operations:\n",
        "* resize to (224, 224), \n",
        "* normalize values to -1, 1, \n",
        "* randomly flip some images, \n",
        "* do not cache the content, \n",
        "* prefetch newer samples (if possible)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hUr44fMAWz6",
        "colab_type": "text"
      },
      "source": [
        "##### Hardware acceleration\n",
        "For the sake of the experiment I used a GPU. You can choose to use a GPU in `Runtime > Change runtime type > GPU`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJXh0DNJjAvw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "33bd27bb-ffac-4774-e628-cfa66582ad98"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Sep 14 18:11:51 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8     7W /  75W |      0MiB /  7611MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4GovedeBn5G",
        "colab_type": "text"
      },
      "source": [
        "### Install TFData Image Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oG9QfSewCWby",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "a04bee51-1dd3-4cec-bae2-adfad74a6620"
      },
      "source": [
        "!pip install -e git+git://github.com/sebastian-sz/tfdata-image-loader.git#egg=tfdata-image-loader"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtaining tfdata-image-loader from git+git://github.com/sebastian-sz/tfdata-image-loader.git#egg=tfdata-image-loader\n",
            "  Cloning git://github.com/sebastian-sz/tfdata-image-loader.git to ./src/tfdata-image-loader\n",
            "  Running command git clone -q git://github.com/sebastian-sz/tfdata-image-loader.git /content/src/tfdata-image-loader\n",
            "Installing collected packages: tfdata-image-loader\n",
            "  Running setup.py develop for tfdata-image-loader\n",
            "Successfully installed tfdata-image-loader\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6g-DD5n3RPR",
        "colab_type": "text"
      },
      "source": [
        "After installing the external module, please restart your runtime.   \n",
        "Alternatively you can run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tN_bP9vKe4Ob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "def restart_runtime():\n",
        "  os.kill(os.getpid(), 9)\n",
        "\n",
        "restart_runtime()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHO1EN_bhb16",
        "colab_type": "text"
      },
      "source": [
        "### Download and unpack example dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VECuV91hawW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "f7c8541d-df6f-48ac-b41a-2ba54f3cfc01"
      },
      "source": [
        "!curl https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz |tar xzf -"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  218M  100  218M    0     0  89.1M      0  0:00:02  0:00:02 --:--:-- 89.1M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXNYp_dBhmEd",
        "colab_type": "text"
      },
      "source": [
        "Remove the license file so it doesn't mess up our directory tree structure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zUqQVhXhjfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm flower_photos/LICENSE.txt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzG2cx6aKvZA",
        "colab_type": "text"
      },
      "source": [
        "### Setting up our experiment\n",
        "\n",
        "We are going to define few constants which are going to be helpful during our comparison:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z5xP6uCCDis",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "5ed45a57-1896-40ad-fa6f-2a5b0f0f7076"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tfdata_image_loader import TFDataImageLoader\n",
        "\n",
        "print(f\"Is gpu available: {tf.test.is_gpu_available()}\")\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-ef89b34a3c51>:10: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "Is gpu available: True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdLLM16WKjBu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 10\n",
        "INPUT_SHAPE = (1, 224, 224, 3)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "DATA_PATH = \"./flower_photos/\"\n",
        "NUM_CLASSES = len(os.listdir(DATA_PATH))\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "IMG_HEIGHT = IMG_WIDTH = INPUT_SHAPE[1]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lstmkIydDy8Z",
        "colab_type": "text"
      },
      "source": [
        "### Create Data Loaders \n",
        "\n",
        "We are going to create two data loaders:\n",
        "1. Custom module: `tfdata-image-loader`.\n",
        "2. The default Keras `flow_from_directory`.\n",
        "3. New Keras preprocessing `image folder dataset`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVQARIKqEgGn",
        "colab_type": "text"
      },
      "source": [
        "#### Create TFData Image Loader and load dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7gkGG2VD1Gj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_data(image, label):\n",
        "    return (image / 127.5) - 1, label\n",
        "\n",
        "\n",
        "def augment_data(image, label):\n",
        "    return tf.image.random_flip_left_right(image), label"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1Noe8y_D9B7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "0ac4f82e-7f3a-4e7d-a4f3-cd60862e0a72"
      },
      "source": [
        "train_loader = TFDataImageLoader(\n",
        "    data_path=DATA_PATH,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    pre_process_function=preprocess_data,\n",
        "    augmentation_function=augment_data,\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3670 images, belonging to 5 classes\n",
            "\n",
            "Class names mapping: \n",
            "{'daisy': array([1, 0, 0, 0, 0], dtype=int32), 'dandelion': array([0, 1, 0, 0, 0], dtype=int32), 'roses': array([0, 0, 1, 0, 0], dtype=int32), 'sunflowers': array([0, 0, 0, 1, 0], dtype=int32), 'tulips': array([0, 0, 0, 0, 1], dtype=int32)}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naOko7tWEQ3R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfdata_image_loader_ds = train_loader.load_dataset()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-7-1oUOHYd5",
        "colab_type": "text"
      },
      "source": [
        "#### Create Keras Image Loader:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1kKR3fwHdgG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_image(img):\n",
        "    return (img / 127.5) - 1\n",
        "\n",
        "train_image_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_image,\n",
        "    horizontal_flip=True,\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vYtzmRMH002",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cb85cbad-bb2e-41c7-c7d2-5b14387140a9"
      },
      "source": [
        "keras_data_gen = train_image_generator.flow_from_directory(\n",
        "    directory=DATA_PATH,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3670 images belonging to 5 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPP0jCSEg5iJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_image_count = len(list(pathlib.Path(DATA_PATH).glob('*/*.jpg')))\n",
        "train_steps_per_epoch = np.ceil(train_image_count/BATCH_SIZE)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeInGLpygyie",
        "colab_type": "text"
      },
      "source": [
        "#### Keras Preprocessing `image_dataset_from_directory`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f03MBx4iIOIb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ff4936c-a000-45de-cbec-a3306e8eeae6"
      },
      "source": [
        "image_dataset_from_directory = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  DATA_PATH,\n",
        "  seed=SEED,\n",
        "  image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "  batch_size=BATCH_SIZE,\n",
        "  label_mode=\"categorical\"\n",
        "  )\n",
        "\n",
        "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(scale=1./127.5, offset=-1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3670 files belonging to 5 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6xb3SbIhYLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_dataset_from_directory = image_dataset_from_directory.map(lambda x, y: (normalization_layer(x), y)).map(augment_data).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Or try without norm layer (similar results):\n",
        "# image_dataset_from_directory = image_dataset_from_directory.map(preprocess_data).map(augment_data).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naZEWM9FLQxE",
        "colab_type": "text"
      },
      "source": [
        "### Example model declaration\n",
        "We are going to use a pretrained MobilenetV2 with a custom head."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCY8CTpxLJ3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(num_classes):\n",
        "  base_model = tf.keras.applications.MobileNetV2(\n",
        "      include_top=False,\n",
        "      pooling=\"avg\",\n",
        "      input_shape=INPUT_SHAPE[1:]\n",
        "  )\n",
        "  base_model.trainable = False\n",
        "\n",
        "  return tf.keras.Sequential([\n",
        "      base_model,\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      tf.keras.layers.Dense(num_classes, activation=\"softmax\")             \n",
        "  ])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJRwVOk_Lbyu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_fresh_model(num_classes):\n",
        "  model = build_model(num_classes=num_classes)\n",
        "  model.compile(\n",
        "      optimizer=tf.keras.optimizers.RMSprop(),\n",
        "      loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "      metrics=[\"accuracy\"]      \n",
        "  )\n",
        "  return model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zp51MrBIylF",
        "colab_type": "text"
      },
      "source": [
        "### Warmup runs\n",
        "\n",
        "We are going to make 3 epochs warm up before we make time measurements:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1D4SkR6LwOM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "5d6ea06a-8f84-4728-b50a-737616934bef"
      },
      "source": [
        "model = build_fresh_model(NUM_CLASSES)\n",
        "model.fit(tfdata_image_loader_ds, epochs=3)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n",
            "Epoch 1/3\n",
            "115/115 [==============================] - 10s 85ms/step - loss: 0.6985 - accuracy: 0.7436\n",
            "Epoch 2/3\n",
            "115/115 [==============================] - 10s 83ms/step - loss: 0.3903 - accuracy: 0.8676\n",
            "Epoch 3/3\n",
            "115/115 [==============================] - 9s 83ms/step - loss: 0.3001 - accuracy: 0.8967\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fba9820ee10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqdcKG-dy5di",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "9a199a21-0d9e-4b2d-c2b5-8beb1c07a8a4"
      },
      "source": [
        "model = build_fresh_model(NUM_CLASSES)\n",
        "model.fit(\n",
        "    keras_data_gen,\n",
        "    steps_per_epoch=train_steps_per_epoch,\n",
        "    epochs=3,\n",
        "    )"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "115/115 [==============================] - 15s 133ms/step - loss: 0.6926 - accuracy: 0.7450\n",
            "Epoch 2/3\n",
            "115/115 [==============================] - 15s 133ms/step - loss: 0.3739 - accuracy: 0.8738\n",
            "Epoch 3/3\n",
            "115/115 [==============================] - 15s 133ms/step - loss: 0.3057 - accuracy: 0.8891\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fba96e15940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frIs417qiJPa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "f7ad0acb-ac5e-4b42-c14a-d02e1c890fb1"
      },
      "source": [
        "model = build_fresh_model(NUM_CLASSES)\n",
        "model.fit(\n",
        "    image_dataset_from_directory,\n",
        "    epochs=3,\n",
        "    )"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "115/115 [==============================] - 12s 105ms/step - loss: 0.7051 - accuracy: 0.7452\n",
            "Epoch 2/3\n",
            "115/115 [==============================] - 12s 103ms/step - loss: 0.3749 - accuracy: 0.8640\n",
            "Epoch 3/3\n",
            "115/115 [==============================] - 12s 105ms/step - loss: 0.3192 - accuracy: 0.8880\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fba04ed4cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpnIDfO-2nv0",
        "colab_type": "text"
      },
      "source": [
        "### Measure performance\n",
        "\n",
        "After warm up runs we are going to measure the performance of two loaders:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcWZhAla2vCu",
        "colab_type": "text"
      },
      "source": [
        "##### Measure time of my TFData Image Loader "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyiD-m3x3OTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_fresh_model(NUM_CLASSES)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX3kLPK32t_9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "f4f75ca4-e274-4563-dbbb-1a9d4fc825e1"
      },
      "source": [
        "start = time.perf_counter()\n",
        "_ = model.fit(tfdata_image_loader_ds, epochs=EPOCHS)\n",
        "\n",
        "print(f\"Train job took: {time.perf_counter() - start} seconds.\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "115/115 [==============================] - 9s 79ms/step - loss: 0.7097 - accuracy: 0.7417\n",
            "Epoch 2/10\n",
            "115/115 [==============================] - 9s 79ms/step - loss: 0.3682 - accuracy: 0.8719\n",
            "Epoch 3/10\n",
            "115/115 [==============================] - 9s 79ms/step - loss: 0.3066 - accuracy: 0.8913\n",
            "Epoch 4/10\n",
            "115/115 [==============================] - 9s 77ms/step - loss: 0.2712 - accuracy: 0.9090\n",
            "Epoch 5/10\n",
            "115/115 [==============================] - 9s 79ms/step - loss: 0.2435 - accuracy: 0.9147\n",
            "Epoch 6/10\n",
            "115/115 [==============================] - 9s 78ms/step - loss: 0.2245 - accuracy: 0.9237\n",
            "Epoch 7/10\n",
            "115/115 [==============================] - 9s 77ms/step - loss: 0.2133 - accuracy: 0.9245\n",
            "Epoch 8/10\n",
            "115/115 [==============================] - 9s 78ms/step - loss: 0.1932 - accuracy: 0.9338\n",
            "Epoch 9/10\n",
            "115/115 [==============================] - 9s 78ms/step - loss: 0.1846 - accuracy: 0.9403\n",
            "Epoch 10/10\n",
            "115/115 [==============================] - 9s 80ms/step - loss: 0.1774 - accuracy: 0.9420\n",
            "Train job took: 94.29033478099996 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX4qCczV3P6p",
        "colab_type": "text"
      },
      "source": [
        "##### Measure time of the default keras `flow_from_directory`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZQ08lLw3TSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_fresh_model(NUM_CLASSES)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ytFjwIj3Uie",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "440cb6e0-7c1b-4fc5-f196-499a57b951a2"
      },
      "source": [
        "start = time.perf_counter()\n",
        "_ = model.fit(\n",
        "    keras_data_gen, \n",
        "    steps_per_epoch=train_steps_per_epoch,    \n",
        "    epochs=EPOCHS\n",
        "    )\n",
        "\n",
        "print(f\"Train job took: {time.perf_counter() - start} seconds.\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "115/115 [==============================] - 15s 132ms/step - loss: 0.6864 - accuracy: 0.7520\n",
            "Epoch 2/10\n",
            "115/115 [==============================] - 15s 131ms/step - loss: 0.3790 - accuracy: 0.8676\n",
            "Epoch 3/10\n",
            "115/115 [==============================] - 15s 128ms/step - loss: 0.3143 - accuracy: 0.8951\n",
            "Epoch 4/10\n",
            "115/115 [==============================] - 15s 127ms/step - loss: 0.2700 - accuracy: 0.9054\n",
            "Epoch 5/10\n",
            "115/115 [==============================] - 15s 128ms/step - loss: 0.2522 - accuracy: 0.9134\n",
            "Epoch 6/10\n",
            "115/115 [==============================] - 15s 128ms/step - loss: 0.2267 - accuracy: 0.9229\n",
            "Epoch 7/10\n",
            "115/115 [==============================] - 15s 129ms/step - loss: 0.2125 - accuracy: 0.9278\n",
            "Epoch 8/10\n",
            "115/115 [==============================] - 15s 127ms/step - loss: 0.2001 - accuracy: 0.9332\n",
            "Epoch 9/10\n",
            "115/115 [==============================] - 15s 127ms/step - loss: 0.1869 - accuracy: 0.9403\n",
            "Epoch 10/10\n",
            "115/115 [==============================] - 15s 130ms/step - loss: 0.1795 - accuracy: 0.9338\n",
            "Train job took: 152.08663721999994 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3555YoWiWW3",
        "colab_type": "text"
      },
      "source": [
        "##### Measure time of the keras.preprocessing `image_dataset_from_directory` loader:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyKqT6N3ihWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_fresh_model(NUM_CLASSES)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC1bb5m_iiix",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "7b433958-60aa-4b58-ae53-49eb24d718f4"
      },
      "source": [
        "start = time.perf_counter()\n",
        "_ = model.fit(\n",
        "    image_dataset_from_directory, \n",
        "    steps_per_epoch=train_steps_per_epoch,    \n",
        "    epochs=EPOCHS\n",
        "    )\n",
        "\n",
        "print(f\"Train job took: {time.perf_counter() - start} seconds.\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "115/115 [==============================] - 12s 102ms/step - loss: 0.7068 - accuracy: 0.7458\n",
            "Epoch 2/10\n",
            "115/115 [==============================] - 12s 106ms/step - loss: 0.3725 - accuracy: 0.8651\n",
            "Epoch 3/10\n",
            "115/115 [==============================] - 12s 104ms/step - loss: 0.3077 - accuracy: 0.8910\n",
            "Epoch 4/10\n",
            "115/115 [==============================] - 12s 108ms/step - loss: 0.2727 - accuracy: 0.9068\n",
            "Epoch 5/10\n",
            "115/115 [==============================] - 12s 102ms/step - loss: 0.2430 - accuracy: 0.9144\n",
            "Epoch 6/10\n",
            "115/115 [==============================] - 12s 101ms/step - loss: 0.2230 - accuracy: 0.9243\n",
            "Epoch 7/10\n",
            "115/115 [==============================] - 12s 100ms/step - loss: 0.2018 - accuracy: 0.9330\n",
            "Epoch 8/10\n",
            "115/115 [==============================] - 12s 100ms/step - loss: 0.1879 - accuracy: 0.9327\n",
            "Epoch 9/10\n",
            "115/115 [==============================] - 12s 101ms/step - loss: 0.1860 - accuracy: 0.9373\n",
            "Epoch 10/10\n",
            "115/115 [==============================] - 12s 102ms/step - loss: 0.1869 - accuracy: 0.9381\n",
            "Train job took: 127.18798694000009 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzCDIeik308F",
        "colab_type": "text"
      },
      "source": [
        "### Conclusions\n",
        "\n",
        "Using MobilenetV2 as feature extractor for a new classification problem, the time to perform the train job took respectively:\n",
        "1. 152.09 seconds, when using default Keras loader\n",
        "2. 127.19 seconds, when using keras.preprocessing `image_dataset_from_directory`\n",
        "3. 94.29 seconds, when using `tfdata-image-loader` module\n",
        "\n",
        "The option to use my implementation provided best results in the above use case."
      ]
    }
  ]
}