{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tfdata_loader_colab_benchmark_loaders.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrooAAAqBswK"
      },
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i52jJdVRAO7p"
      },
      "source": [
        "##### Experiment setup:\n",
        "In this notebook I'm presenting a time benchmark which compares the three options to load images for Image Classification task:\n",
        "1. `tfdata-image-loader` (my repo) \n",
        "2. The default Keras `flow_from_directory` option,\n",
        "3. The new keras.preprocessing `image_dataset_from_directory`\n",
        "\n",
        "In order to perform the measurements I'm fine tuning a MobilenetV2 head on a new set of classes.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoYSMbdhua4F"
      },
      "source": [
        "##### Experiment assumptions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wi4DUa2ufIM"
      },
      "source": [
        "In order to train MobilenetV2 we want to to the following operations:\n",
        "* resize to (224, 224), \n",
        "* normalize values to -1, 1, \n",
        "* randomly flip some images, \n",
        "* do not cache the content, \n",
        "* prefetch newer samples (if possible)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hUr44fMAWz6"
      },
      "source": [
        "##### Hardware acceleration\n",
        "For the sake of the experiment I used a GPU. You can choose to use a GPU in `Runtime > Change runtime type > GPU`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJXh0DNJjAvw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afd7295d-1f24-4150-f23d-7585150942b9"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Mar 19 08:48:26 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4GovedeBn5G"
      },
      "source": [
        "### Install TFData Image Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oG9QfSewCWby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe10acea-23aa-4407-be92-625ae916f093"
      },
      "source": [
        "!pip install -e git+git://github.com/sebastian-sz/tfdata-image-loader.git#egg=tfdata-image-loader"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtaining tfdata-image-loader from git+git://github.com/sebastian-sz/tfdata-image-loader.git#egg=tfdata-image-loader\n",
            "  Cloning git://github.com/sebastian-sz/tfdata-image-loader.git to ./src/tfdata-image-loader\n",
            "  Running command git clone -q git://github.com/sebastian-sz/tfdata-image-loader.git /content/src/tfdata-image-loader\n",
            "Installing collected packages: tfdata-image-loader\n",
            "  Running setup.py develop for tfdata-image-loader\n",
            "Successfully installed tfdata-image-loader\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6g-DD5n3RPR"
      },
      "source": [
        "After installing the external module, please restart your runtime.   \n",
        "Alternatively you can run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tN_bP9vKe4Ob"
      },
      "source": [
        "import os\n",
        "\n",
        "def restart_runtime():\n",
        "  os.kill(os.getpid(), 9)\n",
        "\n",
        "restart_runtime()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHO1EN_bhb16"
      },
      "source": [
        "### Download and unpack example dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VECuV91hawW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "701b1d81-db0a-47b2-cd91-d4dee4cbc2fb"
      },
      "source": [
        "!curl https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz |tar xzf -"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  218M  100  218M    0     0  94.2M      0  0:00:02  0:00:02 --:--:-- 94.2M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXNYp_dBhmEd"
      },
      "source": [
        "Remove the license file so it doesn't mess up our directory tree structure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zUqQVhXhjfB"
      },
      "source": [
        "!rm flower_photos/LICENSE.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzG2cx6aKvZA"
      },
      "source": [
        "### Setting up our experiment\n",
        "\n",
        "We are going to define few constants which are going to be helpful during our comparison:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z5xP6uCCDis",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "50eb06eb-6c2c-4261-8ab3-af8b810a0e7c"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tfdata_image_loader import TFDataImageLoader\n",
        "\n",
        "print(f\"Is gpu available: {tf.test.is_gpu_available()}\")\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-ef89b34a3c51>:10: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "Is gpu available: True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdLLM16WKjBu"
      },
      "source": [
        "EPOCHS = 10\n",
        "INPUT_SHAPE = (1, 224, 224, 3)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "DATA_PATH = \"./flower_photos/\"\n",
        "NUM_CLASSES = len(os.listdir(DATA_PATH))\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "IMG_HEIGHT = IMG_WIDTH = INPUT_SHAPE[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lstmkIydDy8Z"
      },
      "source": [
        "### Create Data Loaders \n",
        "\n",
        "We are going to create two data loaders:\n",
        "1. Custom module: `tfdata-image-loader`.\n",
        "2. The default Keras `flow_from_directory`.\n",
        "3. New Keras preprocessing `image folder dataset`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVQARIKqEgGn"
      },
      "source": [
        "#### Create TFData Image Loader and load dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7gkGG2VD1Gj"
      },
      "source": [
        "def preprocess_data(image, label):\n",
        "    return (image / 127.5) - 1, label\n",
        "\n",
        "\n",
        "def augment_data(image, label):\n",
        "    return tf.image.random_flip_left_right(image), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1Noe8y_D9B7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b778364e-e704-4802-ca99-dcb85585df1f"
      },
      "source": [
        "train_loader = TFDataImageLoader(\n",
        "    data_path=DATA_PATH,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    pre_process_function=preprocess_data,\n",
        "    augmentation_function=augment_data,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3670 images, belonging to 5 classes\n",
            "\n",
            "Class names mapping: \n",
            "{'daisy': array([1, 0, 0, 0, 0], dtype=int32), 'dandelion': array([0, 1, 0, 0, 0], dtype=int32), 'roses': array([0, 0, 1, 0, 0], dtype=int32), 'sunflowers': array([0, 0, 0, 1, 0], dtype=int32), 'tulips': array([0, 0, 0, 0, 1], dtype=int32)}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naOko7tWEQ3R"
      },
      "source": [
        "tfdata_image_loader_ds = train_loader.load_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-7-1oUOHYd5"
      },
      "source": [
        "#### Create Keras Image Loader:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1kKR3fwHdgG"
      },
      "source": [
        "def preprocess_image(img):\n",
        "    return (img / 127.5) - 1\n",
        "\n",
        "train_image_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_image,\n",
        "    horizontal_flip=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vYtzmRMH002",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08a8603c-2ae7-4d18-eaf3-c11edd9bcce2"
      },
      "source": [
        "keras_data_gen = train_image_generator.flow_from_directory(\n",
        "    directory=DATA_PATH,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3670 images belonging to 5 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPP0jCSEg5iJ"
      },
      "source": [
        "train_image_count = len(list(pathlib.Path(DATA_PATH).glob('*/*.jpg')))\n",
        "train_steps_per_epoch = np.ceil(train_image_count/BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeInGLpygyie"
      },
      "source": [
        "#### Keras Preprocessing `image_dataset_from_directory`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f03MBx4iIOIb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18039c51-8c01-4cc0-b573-1d81422520cb"
      },
      "source": [
        "image_dataset_from_directory = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  DATA_PATH,\n",
        "  seed=SEED,\n",
        "  image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "  batch_size=BATCH_SIZE,\n",
        "  label_mode=\"categorical\"\n",
        "  )\n",
        "\n",
        "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(scale=1./127.5, offset=-1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3670 files belonging to 5 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6xb3SbIhYLB"
      },
      "source": [
        "image_dataset_from_directory = image_dataset_from_directory.map(lambda x, y: (normalization_layer(x), y)).map(augment_data).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Or try without norm layer (similar results):\n",
        "# image_dataset_from_directory = image_dataset_from_directory.map(preprocess_data).map(augment_data).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naZEWM9FLQxE"
      },
      "source": [
        "### Example model declaration\n",
        "We are going to use a pretrained MobilenetV2 with a custom head."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCY8CTpxLJ3h"
      },
      "source": [
        "def build_model(num_classes):\n",
        "  base_model = tf.keras.applications.MobileNetV2(\n",
        "      include_top=False,\n",
        "      pooling=\"avg\",\n",
        "      input_shape=INPUT_SHAPE[1:]\n",
        "  )\n",
        "  base_model.trainable = False\n",
        "\n",
        "  return tf.keras.Sequential([\n",
        "      base_model,\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      tf.keras.layers.Dense(num_classes, activation=\"softmax\")             \n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJRwVOk_Lbyu"
      },
      "source": [
        "def build_fresh_model(num_classes):\n",
        "  model = build_model(num_classes=num_classes)\n",
        "  model.compile(\n",
        "      optimizer=tf.keras.optimizers.RMSprop(),\n",
        "      loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "      metrics=[\"accuracy\"]      \n",
        "  )\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zp51MrBIylF"
      },
      "source": [
        "### Warmup runs\n",
        "\n",
        "We are going to make 3 epochs warm up before we make time measurements:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1D4SkR6LwOM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c2249d9-4f13-4271-81cd-873ba923d10a"
      },
      "source": [
        "model = build_fresh_model(NUM_CLASSES)\n",
        "model.fit(tfdata_image_loader_ds, epochs=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n",
            "Epoch 1/3\n",
            "115/115 [==============================] - 42s 65ms/step - loss: 0.9908 - accuracy: 0.6281\n",
            "Epoch 2/3\n",
            "115/115 [==============================] - 7s 62ms/step - loss: 0.3911 - accuracy: 0.8697\n",
            "Epoch 3/3\n",
            "115/115 [==============================] - 7s 61ms/step - loss: 0.3057 - accuracy: 0.8963\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd47e511b10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqdcKG-dy5di",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a37875f5-df04-4e7e-fc23-77edf121bf47"
      },
      "source": [
        "model = build_fresh_model(NUM_CLASSES)\n",
        "model.fit(\n",
        "    keras_data_gen,\n",
        "    steps_per_epoch=train_steps_per_epoch,\n",
        "    epochs=3,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "115/115 [==============================] - 16s 122ms/step - loss: 0.9367 - accuracy: 0.6574\n",
            "Epoch 2/3\n",
            "115/115 [==============================] - 14s 120ms/step - loss: 0.3886 - accuracy: 0.8610\n",
            "Epoch 3/3\n",
            "115/115 [==============================] - 14s 121ms/step - loss: 0.2976 - accuracy: 0.8926\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd48bd50990>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frIs417qiJPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c91ac757-be1c-4cbd-ece7-06902f0acc28"
      },
      "source": [
        "model = build_fresh_model(NUM_CLASSES)\n",
        "model.fit(\n",
        "    image_dataset_from_directory,\n",
        "    epochs=3,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "115/115 [==============================] - 14s 96ms/step - loss: 1.0000 - accuracy: 0.6224\n",
            "Epoch 2/3\n",
            "115/115 [==============================] - 11s 95ms/step - loss: 0.3862 - accuracy: 0.8594\n",
            "Epoch 3/3\n",
            "115/115 [==============================] - 12s 97ms/step - loss: 0.3266 - accuracy: 0.8857\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd48aab4c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpnIDfO-2nv0"
      },
      "source": [
        "### Measure performance\n",
        "\n",
        "After warm up runs we are going to measure the performance of two loaders:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcWZhAla2vCu"
      },
      "source": [
        "##### Measure time of my TFData Image Loader "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyiD-m3x3OTq"
      },
      "source": [
        "model = build_fresh_model(NUM_CLASSES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX3kLPK32t_9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "121c1a57-73e2-497f-a79f-b40b3c3e1c50"
      },
      "source": [
        "start = time.perf_counter()\n",
        "_ = model.fit(tfdata_image_loader_ds, epochs=EPOCHS)\n",
        "\n",
        "print(f\"Train job took: {time.perf_counter() - start} seconds.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "115/115 [==============================] - 9s 62ms/step - loss: 1.0428 - accuracy: 0.6148\n",
            "Epoch 2/10\n",
            "115/115 [==============================] - 7s 63ms/step - loss: 0.3861 - accuracy: 0.8627\n",
            "Epoch 3/10\n",
            "115/115 [==============================] - 7s 64ms/step - loss: 0.3070 - accuracy: 0.8980\n",
            "Epoch 4/10\n",
            "115/115 [==============================] - 7s 62ms/step - loss: 0.2650 - accuracy: 0.9106\n",
            "Epoch 5/10\n",
            "115/115 [==============================] - 7s 62ms/step - loss: 0.2355 - accuracy: 0.9135\n",
            "Epoch 6/10\n",
            "115/115 [==============================] - 7s 63ms/step - loss: 0.2206 - accuracy: 0.9282\n",
            "Epoch 7/10\n",
            "115/115 [==============================] - 7s 63ms/step - loss: 0.2111 - accuracy: 0.9233\n",
            "Epoch 8/10\n",
            "115/115 [==============================] - 7s 63ms/step - loss: 0.1887 - accuracy: 0.9349\n",
            "Epoch 9/10\n",
            "115/115 [==============================] - 7s 63ms/step - loss: 0.1848 - accuracy: 0.9382\n",
            "Epoch 10/10\n",
            "115/115 [==============================] - 7s 63ms/step - loss: 0.1653 - accuracy: 0.9498\n",
            "Train job took: 75.36934199800004 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX4qCczV3P6p"
      },
      "source": [
        "##### Measure time of the default keras `flow_from_directory`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZQ08lLw3TSf"
      },
      "source": [
        "model = build_fresh_model(NUM_CLASSES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ytFjwIj3Uie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec8d1470-23af-4980-eae0-5b08d3ccf61a"
      },
      "source": [
        "start = time.perf_counter()\n",
        "_ = model.fit(\n",
        "    keras_data_gen, \n",
        "    steps_per_epoch=train_steps_per_epoch,    \n",
        "    epochs=EPOCHS\n",
        "    )\n",
        "\n",
        "print(f\"Train job took: {time.perf_counter() - start} seconds.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "115/115 [==============================] - 16s 122ms/step - loss: 0.9777 - accuracy: 0.6337\n",
            "Epoch 2/10\n",
            "115/115 [==============================] - 14s 121ms/step - loss: 0.4121 - accuracy: 0.8527\n",
            "Epoch 3/10\n",
            "115/115 [==============================] - 14s 122ms/step - loss: 0.3165 - accuracy: 0.8890\n",
            "Epoch 4/10\n",
            "115/115 [==============================] - 14s 120ms/step - loss: 0.2657 - accuracy: 0.9126\n",
            "Epoch 5/10\n",
            "115/115 [==============================] - 14s 120ms/step - loss: 0.2404 - accuracy: 0.9214\n",
            "Epoch 6/10\n",
            "115/115 [==============================] - 14s 121ms/step - loss: 0.2196 - accuracy: 0.9251\n",
            "Epoch 7/10\n",
            "115/115 [==============================] - 14s 120ms/step - loss: 0.2243 - accuracy: 0.9179\n",
            "Epoch 8/10\n",
            "115/115 [==============================] - 14s 123ms/step - loss: 0.1923 - accuracy: 0.9366\n",
            "Epoch 9/10\n",
            "115/115 [==============================] - 14s 120ms/step - loss: 0.1729 - accuracy: 0.9468\n",
            "Epoch 10/10\n",
            "115/115 [==============================] - 14s 120ms/step - loss: 0.1781 - accuracy: 0.9393\n",
            "Train job took: 141.661524451 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3555YoWiWW3"
      },
      "source": [
        "##### Measure time of the keras.preprocessing `image_dataset_from_directory` loader:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyKqT6N3ihWn"
      },
      "source": [
        "model = build_fresh_model(NUM_CLASSES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC1bb5m_iiix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b81b8259-4c15-4197-e388-a071ca407b8d"
      },
      "source": [
        "start = time.perf_counter()\n",
        "_ = model.fit(\n",
        "    image_dataset_from_directory, \n",
        "    steps_per_epoch=train_steps_per_epoch,    \n",
        "    epochs=EPOCHS\n",
        "    )\n",
        "\n",
        "print(f\"Train job took: {time.perf_counter() - start} seconds.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "115/115 [==============================] - 14s 97ms/step - loss: 1.0087 - accuracy: 0.6178\n",
            "Epoch 2/10\n",
            "115/115 [==============================] - 12s 97ms/step - loss: 0.3816 - accuracy: 0.8583\n",
            "Epoch 3/10\n",
            "115/115 [==============================] - 11s 95ms/step - loss: 0.3258 - accuracy: 0.8843\n",
            "Epoch 4/10\n",
            "115/115 [==============================] - 12s 96ms/step - loss: 0.2794 - accuracy: 0.9042\n",
            "Epoch 5/10\n",
            "115/115 [==============================] - 11s 95ms/step - loss: 0.2463 - accuracy: 0.9106\n",
            "Epoch 6/10\n",
            "115/115 [==============================] - 11s 95ms/step - loss: 0.2283 - accuracy: 0.9220\n",
            "Epoch 7/10\n",
            "115/115 [==============================] - 11s 95ms/step - loss: 0.1994 - accuracy: 0.9314\n",
            "Epoch 8/10\n",
            "115/115 [==============================] - 11s 95ms/step - loss: 0.1966 - accuracy: 0.9260\n",
            "Epoch 9/10\n",
            "115/115 [==============================] - 11s 95ms/step - loss: 0.1987 - accuracy: 0.9306\n",
            "Epoch 10/10\n",
            "115/115 [==============================] - 11s 96ms/step - loss: 0.1918 - accuracy: 0.9377\n",
            "Train job took: 117.33423165699998 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzCDIeik308F"
      },
      "source": [
        "### Conclusions\n",
        "\n",
        "Using MobilenetV2 as feature extractor for a new classification problem, the time to perform the train job took respectively:\n",
        "1. 141.66 seconds, when using default Keras loader\n",
        "2. 117.33 seconds, when using keras.preprocessing `image_dataset_from_directory`\n",
        "3. 75.37 seconds, when using `tfdata-image-loader` module\n",
        "\n",
        "The option to use my implementation provided best results in the above use case."
      ]
    }
  ]
}
